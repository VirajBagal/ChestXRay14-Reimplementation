{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chestx-ray14-training-ae-cnn.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"rMfgY7yU5Q-y","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"qZFBQtTd5I_Z","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd \n","import os\n","from glob import glob\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cv2\n","from tqdm.notebook import tqdm\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","import ast\n","\n","from torch.autograd import Function\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as tfm\n","from torchvision import models\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.nn import MSELoss\n","import torch\n","import random\n","from torch import nn\n","\n","plt.style.use('default')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"42iRij-E5I_g","colab_type":"code","colab":{}},"source":["EPOCHS = 20\n","DEBUG = True\n","RESUME = False\n","LR = 1e-4\n","WD = 1e-5\n","PAT = 5\n","FACTOR = 0.1\n","BS = 16\n","NUM_CLASSES = 14\n","CROP_SIZE = 896\n","SEED = 42\n","lam = 0.9     #used in loss function\n","\n","if DEBUG:\n","    EPOCHS = 1\n","\n","DIR = '/content/gdrive/My Drive/ChestXray14/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Dz6dtOWF5I_k","colab_type":"code","colab":{}},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n"," \n","seed_everything(SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"tB_LSPiZ5I_o","colab_type":"code","colab":{}},"source":["invalid = pd.read_csv('../input/chestxray14-invalid/invalid.txt', sep=' ', header=None)\n","invalid_list = []\n","\n","invalid_list.append(ast.literal_eval(invalid.values[0][0][1:-1]).split('/')[-1])\n","for i in range(518):\n","  invalid_list.append(ast.literal_eval(invalid.values[0][i+1][:-1]).split('/')[-1])  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"3TH8B0Ac5I_s","colab_type":"code","colab":{}},"source":["train_idx = np.concatenate(pd.read_csv('../input/chestxray14-csvs/train_val_list.txt', sep=' ', header=None).values)\n","test_idx = np.concatenate(pd.read_csv('../input/chestxray14-csvs/test_list.txt', sep=' ', header=None).values)\n","\n","# taken from: https://www.kaggle.com/kmader/train-simple-xray-cnn/input?select=Data_Entry_2017.csv\n","\n","all_xray_df = pd.read_csv('../input/csv-for-chest/Data_Entry_2017_v2020.csv')\n","all_image_paths = {os.path.basename(x): x for x in \n","                   glob(os.path.join('..', 'input', 'data','*','images', '*.png'))}\n","print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n","all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n","all_xray_df.sample(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"snt6BPXt5I_v","colab_type":"code","colab":{}},"source":["# taken from: https://www.kaggle.com/kmader/train-simple-xray-cnn/input?select=Data_Entry_2017.csv\n","\n","all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n","from itertools import chain\n","all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n","all_labels = [x for x in all_labels if len(x)>0]\n","print('All Labels ({}): {}'.format(len(all_labels), all_labels))\n","for c_label in all_labels:\n","    if len(c_label)>1: # leave out empty labels\n","        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n","        \n","all_xray_df.sample(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"KCBY_aT35I_z","colab_type":"code","colab":{}},"source":["class ChestDataset(Dataset):\n","    def __init__(self, df, transform, mode='train'):\n","        self.label_df = df[all_labels].values\n","        self.path_df = df['path'].values\n","        self.transform = transform\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.path_df)\n","    \n","    def __getitem__(self, idx):\n","        path = self.path_df[idx]\n","        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n","        \n","        if self.mode=='val':\n","            img = cv2.resize(img, (CROP_SIZE, CROP_SIZE))\n","            \n","#         img = np.stack([img,img,img], -1)\n","        \n","        if self.transform:\n","            img = self.transform(img)\n","            \n","\n","        label = self.label_df[idx]\n","            \n","\n","        return img, torch.tensor(label, dtype=torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Fw-h11PS5I_2","colab_type":"code","colab":{}},"source":["train_tfm = tfm.Compose([tfm.ToPILImage(),\n","                         tfm.RandomCrop(CROP_SIZE),\n","                         tfm.RandomRotation(5),\n","                         tfm.RandomHorizontalFlip(),\n","                         tfm.ToTensor()])\n","\n","val_tfm = tfm.Compose([tfm.ToPILImage(),\n","                       tfm.ToTensor()])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"haCFwZRj5I_6","colab_type":"code","colab":{}},"source":["all_xray_df = all_xray_df[~all_xray_df['Image Index'].isin(invalid_list)].reset_index(drop=True)\n","train_valdf = all_xray_df[all_xray_df['Image Index'].isin(train_idx)].reset_index(drop=True)\n","valdf = train_valdf.sample(n=10000, random_state=SEED)\n","traindf = train_valdf[~train_valdf['Image Index'].isin(list(valdf['Image Index'].values))]\n","testdf = all_xray_df[all_xray_df['Image Index'].isin(test_idx)].reset_index(drop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"tQzGPbz85I_8","colab_type":"code","colab":{}},"source":["train_dataset = ChestDataset(traindf, train_tfm)\n","val_dataset = ChestDataset(valdf, val_tfm, mode='val')\n","\n","trainloader = DataLoader(train_dataset, batch_size=BS, num_workers=4, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=BS, num_workers=4, shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"vsuR95fs5I__","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(20,10))\n","\n","for i in range(15):\n","    plt.subplot(3,5,i+1)\n","    idx = np.random.randint(len(trainloader))\n","    img, _ = train_dataset[idx]\n","    plt.imshow(img.numpy().transpose(1,2,0).squeeze(-1), cmap='gray')\n","    plt.axis('off')\n","    \n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"2AzrAhku5JAE","colab_type":"code","colab":{}},"source":["class AECNN(nn.Module):\n","\n","    def __init__(self, classCount):\n","        super (AECNN0, self).__init__()\n","\n","        self.classCount = classCount\n","        # self.y2 = torch.Tensor(bs, 3, h, w).cuda()\n","        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","        self.encoder = nn.Sequential(\n","            #1x896x896\n","            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5, stride = 4, padding = 2),\n","            nn.ELU(),\n","            #1X224X224\n","            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1, stride=1, padding=0),\n","            #1x224x224\n","            )\n","\n","        self.decoder = nn.Sequential(\n","            #1x224x224\n","            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n","            nn.PixelShuffle(4),\n","            #1x896x896\n","            )\n","\n","\n","        #CLASSIFIER\n","        self.classifier = DenseNet121(classCount = self.classCount, isTrained = True)\n","\n","    def forward(self, x):\n","        \n","        y = self.encoder(x)\n","        y = Relu1.apply(y)\n","        \n","        z1 = self.decoder(y)\n","        z1 = Relu1.apply(z1)\n","        \n","        bs, c, h, w = y.shape\n","        y2 = torch.Tensor(bs, 3, h, w).cuda()\n","        \n","        for img_no in range(bs):\n","            y2[img_no] = y[img_no]\n","            y2[img_no] = self.normalize(y2[img_no]) #broadcasting 1 channel to 3 channels\n","\n","        z2 = self.classifier(y2)\n","\n","        return z1, z2\n","\n","\n","\n","class Relu1(Function):\n","\n","    @staticmethod\n","    def forward(ctx, input):\n","\n","        ctx.save_for_backward(input)\n","        #print(\"fwd:\", input[0])\n","        return input.clamp(min=0, max=1)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","\n","        input, = ctx.saved_tensors\n","        grad_input = grad_output.clone()\n","        grad_input[input<0]*=0.0\n","        grad_input[input>1]*=0.0\n","\n","        return grad_input"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"VTzDF6pF5JAI","colab_type":"code","colab":{}},"source":["model = AECNN()\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay = WD)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=PAT, factor = FACTOR, mode='min') # Reduce lr by 0.1 factor after every 5 epochs\n","criterion = MSELoss()\n","\n","def get_lr(optimizer):\n","  lr_list = []\n","  for p in optimizer.param_groups:\n","    lr_list.append(p['lr'])\n","\n","  return lr_list[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"H4r5Sz5m5JAL","colab_type":"code","colab":{}},"source":["model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ju4qrBxp5JAN","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_one_epoch(epoch, dataloader, model, criterion, optimizer, scheduler):\n","  train_loss = 0\n","  length = len(dataloader)\n","  model.train()\n","  optimizer.zero_grad()\n","  iterator = tqdm(enumerate(dataloader), total=length, leave=False, desc=f'Epoch {epoch+1}/{EPOCHS}')\n","\n","  for i, (img, label) in iterator:\n","    img = img.to(device)\n","    label = label.to(device)\n","    \n","    ae_img, pred_label = model(img)\n","    \n","    img_loss = criterion1(ae_img,img)\n","    label_loss = criterion2(pred_label, label)\n","    loss = lam*label_loss + (1-lam)*img_loss\n","    \n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    \n","    train_loss += loss.item()/length\n","\n","\n","    if DEBUG:\n","      if i==100:\n","        break\n","\n","    if scheduler:\n","      scheduler.step()\n","\n","  return train_loss\n","\n","def validate_one_epoch(epoch, dataloader, model, criterion):\n","  val_loss = 0\n","  length = len(dataloader)\n","  model.eval()\n","  iterator = tqdm(enumerate(dataloader), total=length, leave=False, desc=f'Epoch {epoch+1}/{EPOCHS}')\n","\n","  for i, (img, label) in iterator:\n","    img = img.to(device)\n","    label = label.to(device)\n","    \n","    ae_img, pred_label = model(img)\n","\n","    \n","    img_loss = criterion1(ae_img,img)\n","    label_loss = criterion2(pred_label, label)\n","    loss = lam*label_loss + (1-lam)*img_loss\n","    \n","    val_loss += loss.item()/length\n","    \n","    if DEBUG:\n","      if i==100:\n","        break\n","\n","  return val_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"gNbRV0r95JAQ","colab_type":"code","colab":{}},"source":["best_loss = np.inf\n","best_auc = 0\n","\n","model.to(device)\n","\n","for epoch in range(EPOCHS):\n","  if RESUME:\n","    epoch = resume_epoch + 1\n","\n","  t_loss = train_one_epoch(epoch, trainloader, model, criterion, optimizer, scheduler=None)\n","  lr = get_lr(optimizer)\n","  print('Epoch {}/{} (train) || Loss: {:.4f} LR: {:.5f}'.format(epoch+1, EPOCHS, t_loss, lr))\n","    \n","  v_loss = validate_one_epoch(epoch, valloader, model, criterion)\n","  print('Epoch {}/{} (validation) || Loss: {:.4f} '.format(epoch+1, EPOCHS, v_loss))\n","\n","  scheduler.step(v_loss)  \n","\n","\n","  content = 'Train Loss: {:.4f} Val Loss: {:.4f}'.format(t_loss, v_loss)\n","\n","  recorder={}\n","\n","  recorder['epoch'] = epoch\n","  recorder['model'] = model.state_dict()\n","  recorder['optimizer'] = optimizer.state_dict()\n","  recorder['scheduler'] = scheduler.state_dict()\n","\n","  torch.save(recorder, DIR+'chestxray14_ae_recorder.pth')\n","\n","  with open(DIR+'chestxray14_ae.txt', 'a') as logger:\n","    logger.write(content + '\\n')\n","\n","  if v_loss<best_loss:\n","    torch.save(model.state_dict(), DIR+'chestxray14_ae_loss.pth')\n","    best_loss = v_loss"],"execution_count":0,"outputs":[]}]}